# Bad_Models_Literature
A list of notable sources on bad models
References
[1]	S. Salcedo-Sanz, P. Ghamisi, M. Piles, M. Werner, L. Cuadra, A. Moreno-Martínez, E. Izquierdo-Verdiguier, J. Muñoz-Marí, A. Mosavi, G. Camps-Valls, Machine learning information fusion in Earth observation: A comprehensive review of methods, applications and data sources, Information Fusion. (2020). https://doi.org/10.1016/j.inffus.2020.07.004.
[2]	C. Robert,                             Machine Learning, a Probabilistic Perspective                          , CHANCE. (2014). https://doi.org/10.1080/09332480.2014.914768.
[3]	N. Guimarães, R. Campos, A. Jorge, Pre-trained language models: What do they know?, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 14 (2024) pp. e1518. https://doi.org/10.1002/WIDM.1518.
[4]	A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, in: Adv. Neural Inf. Process. Syst., 2017.
[5]	S. Caton, C. Haas, Fairness in Machine Learning: A Survey, ACM Computing Surveys. 56 (2024) pp. 1–38. https://doi.org/10.1145/3616865/SUPPL_FILE/3616865-SUPP.PDF.
[6]	J.S. Hodges, Six (Or So) Things You Can Do with a Bad Model, Operations Research. (1991). https://doi.org/10.1287/opre.39.3.355.
[7]	T. Meng, X. Jing, Z. Yan, W. Pedrycz, A survey on machine learning for data fusion, Information Fusion. (2020). https://doi.org/10.1016/j.inffus.2019.12.001.
[8]	T. McCausland, The Bad Data Problem, Research Technology Management. (2020). https://doi.org/10.1080/08956308.2021.1844540.
[9]	D. Schuster, S.J. van Zelst, W.M.P. van der Aalst, Utilizing domain knowledge in data-driven process discovery: A literature review, Computers in Industry. (2022). https://doi.org/10.1016/j.compind.2022.103612.
[10]	A. Berber, S. Srećković, When something goes wrong: Who is responsible for errors in ML decision-making?, AI and Society. (2024). https://doi.org/10.1007/s00146-023-01640-1.
[11]	J. Shin, M. Wei, J. Wang, L. Shi, S. Wang, The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks, ACM Transactions on Software Engineering and Methodology. (2023). https://doi.org/10.1145/3630009.
[12]	T.S. Sethi, M. Kantardzic, When Good Machine Learning Leads to Bad Security: Big Data (Ubiquity Symposium), Ubiquity. (2018).
[13]	A. Gotmare, N. Shirish Keskar, C. Xiong, R. Socher, A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation, in: 7th Int. Conf. Learn. Represent. ICLR 2019, International Conference on Learning Representations, ICLR, 2019. https://arxiv.org/abs/1810.13243v1 (accessed February 1, 2025).
[14]	S.C. Slota, K.R. Fleischmann, S. Greenberg, N. Verma, B. Cummings, L. Li, C. Shenefiel, Good systems, bad data?: Interpretations of AI hype and failures, Proceedings of the Association for Information Science and Technology. (2020). https://doi.org/10.1002/pra2.275.
[15]	S. Qiu, H. Zhao, N. Jiang, Z. Wang, L. Liu, Y. An, H. Zhao, X. Miao, R. Liu, G. Fortino, Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges, Information Fusion. (2022). https://doi.org/10.1016/j.inffus.2021.11.006.
[16]	T. Räz, ML interpretability: Simple isn’t easy, Studies in History and Philosophy of Science. (2024). https://doi.org/10.1016/j.shpsa.2023.12.007.
[17]	H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan, Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning, in: Conf. Hum. Factors Comput. Syst. - Proc., 2020. https://doi.org/10.1145/3313831.3376219.
[18]	A. Cropper, R. Morel, Learning programs by learning from failures, Machine Learning. (2021). https://doi.org/10.1007/s10994-020-05934-z.
[19]	M.Z. Naser, A.H. Alavi, Error Metrics and Performance Fitness Indicators for Artificial Intelligence and Machine Learning in Engineering and Sciences, Architecture, Structures and Construction. 1 (2021) pp. 1–19. https://doi.org/https://doi.org/10.1007/s44150-021-00015-8.
[20]	T. Hastie, R. Tibshirani, J.H. Friedman, MyiLibrary., The elements of statistical learning data mining, inference, and prediction : with 200 full-color illustrations, Springer Series in Statistics. (2001).
[21]	A. Chakarov, A. Nori, S. Rajamani B, S. Sen, D. Vijaykeerthy, Debugging Machine Learning Tasks, (2016). https://arxiv.org/abs/1603.07292v1 (accessed February 1, 2025).
[22]	C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, Intriguing properties of neural networks, (n.d.).
[23]	A. Rosenfeld, J.K. Tsotsos, Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing, in: Proc. - 2019 16th Conf. Comput. Robot Vision, CRV 2019, 2019. https://doi.org/10.1109/CRV.2019.00010.
[24]	G. Hinton, O. Vinyals, J. Dean, Distilling the Knowledge in a Neural Network, (2015). https://arxiv.org/abs/1503.02531v1 (accessed February 1, 2025).
[25]	Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning, ACM International Conference Proceeding Series. 382 (2009). https://doi.org/10.1145/1553374.1553380.
[26]	I. Radosavovic, P. Dollar, R. Girshick, G. Gkioxari, K. He, Data Distillation: Towards Omni-Supervised Learning, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2018. https://doi.org/10.1109/CVPR.2018.00433.
[27]	A. Kumar, J. Naughton, J.M. Patel, Learning generalized linear models over normalized data, in: Proc. ACM SIGMOD Int. Conf. Manag. Data, 2015. https://doi.org/10.1145/2723372.2723713.
[28]	K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C.C. Loy, Domain Generalization: A Survey, IEEE Transactions on Pattern Analysis and Machine Intelligence. 45 (2023) pp. 4396–4415. https://doi.org/10.1109/TPAMI.2022.3195549.
[29]	Y. Pan, L. Zhang, X. Wu, M.J. Skibniewski, Multi-classifier information fusion in risk analysis, Information Fusion. (2020). https://doi.org/10.1016/j.inffus.2020.02.003.
[30]	Y. Koren, R. Bell, C. Volinsky, Matrix factorization techniques for recommender systems, Computer. (2009). https://doi.org/10.1109/MC.2009.263.
[31]	E.A. Lee, Constructive models of discrete and continuous physical phenomena, IEEE Access. (2014). https://doi.org/10.1109/ACCESS.2014.2345759.
[32]	M.Z. Naser, Machine Learning for Civil and Environmental Engineers: A Practical Approach to Data-Driven Analysis, Explainability, and Causality, Wiley, New Jersey, 2023.
[33]	L. Ma, H. Kang, G. Yu, Q. Li, Q. He, Single-Domain Generalized Predictor for Neural Architecture Search System, IEEE Transactions on Computers. 73 (2024) pp. 1400–1413. https://doi.org/10.1109/TC.2024.3365949.
[34]	D. Li, Y. Yang, Y.Z. Song, T.M. Hospedales, Deeper, Broader and Artier Domain Generalization, in: Proc. IEEE Int. Conf. Comput. Vis., 2017. https://doi.org/10.1109/ICCV.2017.591.
[35]	B. Hutchinson, N. Rostamzadeh, C. Greer, K. Heller, V. Prabhakaran, Evaluation Gaps in Machine Learning Practice, in: ACM Int. Conf. Proceeding Ser., 2022. https://doi.org/10.1145/3531146.3533233.
[36]	G.E. Karniadakis, I.G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, L. Yang, Physics-informed machine learning, Nature Reviews Physics. (2021). https://doi.org/10.1038/s42254-021-00314-5.
[37]	N. Kovachki, N. Zongyi Li, C. Burigede Liu, K. Azizzadenesheli, N. Kaushik Bhattacharya, A. Stuart, A. Anandkumar, C. Editor, L. Rosasco, Z. Li, B. Liu, K. Bhattacharya, Neural operator, The Journal of Machine Learning Research. 24 (2023) pp. 1–97. https://doi.org/10.5555/3648699.3648788.
[38]	D.J. Abadi, Data Management in the Cloud : Limitations and Opportunities, Bulletin of the IEEE Computer Society Technical Commitee on Data Engineering. (2009).
[39]	E.M. Dos Santos, R. Sabourin, P. Maupin, Overfitting cautious selection of classifier ensembles with genetic algorithms, Information Fusion. (2009). https://doi.org/10.1016/j.inffus.2008.11.003.
[40]	C. Shyalika, R. Wickramarachchi, A. Sheth, A Comprehensive Survey on Rare Event Prediction, ACM Computing Surveys. 57 (2023) pp. 39. https://doi.org/10.1145/3699955/SUPPL_FILE/3699955.PDF.
[41]	A. Nandy, C. Duan, H.J. Kulik, Audacity of huge: overcoming challenges of data scarcity and data quality for machine learning in computational materials discovery, Current Opinion in Chemical Engineering. (2022). https://doi.org/10.1016/j.coche.2021.100778.
[42]	W. Zhang, P. Zhang, B. Zhang, X. Wang, D. Wang, A Collaborative Transfer Learning Framework for Cross-domain Recommendation, in: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., 2023. https://doi.org/10.1145/3580305.3599758.
[43]	J. Wang, C. Shi, Z. Wu, A Robust Test for the Stationarity Assumption in Sequential Decision Making, in: Proc. Mach. Learn. Res., 2023.
[44]	N. Adams, Dataset Shift in Machine Learning, Journal of the Royal Statistical Society Series A: Statistics in Society. (2010). https://doi.org/10.1111/j.1467-985x.2009.00624_10.x.
[45]	A.I. Paganelli, A.G. Mondéjar, A.C. da Silva, G. Silva-Calpa, M.F. Teixeira, F. Carvalho, A. Raposo, M. Endler, Real-time data analysis in health monitoring systems: A comprehensive systematic literature review, Journal of Biomedical Informatics. (2022). https://doi.org/10.1016/j.jbi.2022.104009.
[46]	J. Xu, L. Yao, L. Li, M. Ji, G. Tang, Argumentation based reinforcement learning for meta-knowledge extraction, Information Sciences. (2020). https://doi.org/10.1016/j.ins.2019.07.094.
[47]	D. Abbott, Applied Predictive Analytics. Principles and techniques for the professional data analyst, 2014.
[48]	I.D. Mienye, Y. Sun, A Survey of Ensemble Learning: Concepts, Algorithms, Applications, and Prospects, IEEE Access. (2022). https://doi.org/10.1109/ACCESS.2022.3207287.
[49]	C.Y. Chiu, Y.J. Lee, C.C. Chang, W.Y. Luo, H.C. Huang, Semi-supervised learning for false alarm reduction, in: Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), 2010. https://doi.org/10.1007/978-3-642-14400-4_46.
[50]	N. Gunantara, A review of multi-objective optimization: Methods and its applications, Cogent Engineering. (2018). https://doi.org/10.1080/23311916.2018.1502242.
[51]	S. Prakash, J. Narkarunai, A. Malaiyappan, K. Thirunavukkarasu, M. Devan, Achieving Regulatory Compliance in Cloud Computing through ML, AIJMR - Advanced International Journal of Multidisciplinary Research. 2 (2024). https://doi.org/10.62127/AIJMR.2024.V02I02.1038.
[52]	C. Molnar, Interpretable Machine Learning. A Guide for Making Black Box Models Explainable., Book. (2019).
[53]	J. Su, Y. Chen, T. Cai, T. Wu, R. Gao, L. Wang, J.D. Lee, Sanity-checking pruning methods: Random tickets can win the jackpot, in: Adv. Neural Inf. Process. Syst., 2020.
[54]	Q. Yuan, C. Liu, W. Yu, Y. Zhu, G. Xiong, Y. Wang, G. Gou, BoAu: Malicious traffic detection with noise labels based on boundary augmentation, Computers and Security. (2023). https://doi.org/10.1016/j.cose.2023.103300.
[55]	K. Liu, L. Li, L. Wan, Weighted Null Vector Initialization and its Application to Phase Retrieval, in: ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc., 2020. https://doi.org/10.1109/ICASSP40776.2020.9054331.
[56]	I. Goldenberg, G.I. Webb, PCA-based drift and shift quantification framework for multidimensional data, Knowledge and Information Systems. (2020). https://doi.org/10.1007/s10115-020-01438-3.
[57]	F.E. Eid, H.A. Elmarakeby, Y.A. Chan, N. Fornelos, M. ElHefnawi, E.M. Van Allen, L.S. Heath, K. Lage, Systematic auditing is essential to debiasing machine learning in biology, Communications Biology. (2021). https://doi.org/10.1038/s42003-021-01674-5.
[58]	D. Bzdok, N. Altman, M. Krzywinski, Statistics versus machine learning, Nature Methods. (2018). https://doi.org/10.1038/nmeth.4642.
[59]	T. Neideen, K. Brasel, Understanding Statistical Tests, Journal of Surgical Education. (2007). https://doi.org/10.1016/j.jsurg.2007.02.001.
[60]	V. Muthukumar, A. Narang, V. Subramanian, M. Belkin, D. Hsu, A. Sahai, Classification vs regression in overparameterized regimes: Does the loss function matter?, Journal of Machine Learning Research. (2021).
[61]	T. Schmied, D. Didona, A. Döring, T. Parnell, N. Ioannou, Towards a General Framework for ML-based Self-tuning Databases, in: Proc. 1st Work. Mach. Learn. Syst. EuroMLSys 2021, 2021. https://doi.org/10.1145/3437984.3458830.
[62]	A. Vabalas, E. Gowen, E. Poliakoff, A.J. Casson, Machine learning algorithm validation with a limited sample size, PLoS ONE. (2019). https://doi.org/10.1371/journal.pone.0224365.
[63]	H. Li, Z. Xu, G. Taylor, C. Studer, T. Goldstein, Visualizing the loss landscape of neural nets, in: Adv. Neural Inf. Process. Syst., 2018.
[64]	B. Ghorbani, S. Krishnan, Y. Xiao, An investigation into neural net optimization via Hessian eigenvalue density, in: 36th Int. Conf. Mach. Learn. ICML 2019, 2019.
[65]	F. Bach, R. Jenatton, J. Mairal, G. Obozinski, Convex Optimization with Sparsity-Inducing Norms, in: Optim. Mach. Learn., 2019. https://doi.org/10.7551/mitpress/8996.003.0004.
[66]	M. Iqbal, S.S. Naqvi, W.N. Browne, C. Hollitt, M. Zhang, Learning feature fusion strategies for various image types to detect salient objects, Pattern Recognition. (2016). https://doi.org/10.1016/j.patcog.2016.05.020.
[67]	S. Verma, V. Boonsanong, M. Hoang, K. Hines, J. Dickerson, C. Shah, Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review, ACM Computing Surveys. (2024). https://doi.org/10.1145/3677119/ASSET/2907A6DC-DD42-4AF6-BE48-CA8C296851FC/ASSETS/GRAPHIC/CSUR-2023-0595-T02.JPG.
[68]	S.G. Finlayson, J.D. Bowers, J. Ito, J.L. Zittrain, A.L. Beam, I.S. Kohane, Adversarial attacks on medical machine learning, Science. (2019). https://doi.org/10.1126/science.aaw4399.
[69]	L. Xiong, M. Ye, D. Zhang, Y. Gan, X. Li, Y. Zhu, Source data-free domain adaptation of object detector through domain-specific perturbation, International Journal of Intelligent Systems. (2021). https://doi.org/10.1002/int.22434.
[70]	A.T. Young, K. Fernandez, J. Pfau, R. Reddy, N.A. Cao, M.Y. von Franque, A. Johal, B. V. Wu, R.R. Wu, J.Y. Chen, R.P. Fadadu, J.A. Vasquez, A. Tam, M.J. Keiser, M.L. Wei, Stress testing reveals gaps in clinic readiness of image-based diagnostic artificial intelligence models, Npj Digital Medicine. (2021). https://doi.org/10.1038/s41746-020-00380-6.
[71]	R.D. King, O.I. Orhobor, C.C. Taylor, Cross-validation is safe to use, Nature Machine Intelligence. (2021). https://doi.org/10.1038/s42256-021-00332-z.
[72]	T.T. Wong, P.Y. Yeh, Reliable Accuracy Estimates from k-Fold Cross Validation, IEEE Transactions on Knowledge and Data Engineering. (2020). https://doi.org/10.1109/TKDE.2019.2912815.
[73]	L. Cardoso Silva, F. Rezende Zagatti, B. Silva Sette, L. Nildaimon Dos Santos Silva, D. Lucredio, D. Furtado Silva, H. De Medeiros Caseli, Benchmarking Machine Learning Solutions in Production, in: Proc. - 19th IEEE Int. Conf. Mach. Learn. Appl. ICMLA 2020, 2020. https://doi.org/10.1109/ICMLA51294.2020.00104.
[74]	A. Krizhevsky, V. Nair, G. Hinton, CIFAR-10 and CIFAR-100 datasets, Https://Www.Cs.Toronto.Edu/~Kriz/Cifar.Html. (2009).
[75]	Y. Lu, L. Chen, Y. Zhang, Y. Zhang, B. Han, Y.M. Cheung, H. Wang, Federated Learning with Extremely Noisy Clients via Negative Distillation, Proceedings of the AAAI Conference on Artificial Intelligence. 38 (2024) pp. 14184–14192. https://doi.org/10.1609/AAAI.V38I13.29329.
[76]	L. Liu, R.T. Tan, Certainty driven consistency loss on multi-teacher networks for semi-supervised learning, Pattern Recognition. (2021). https://doi.org/10.1016/j.patcog.2021.108140.
[77]	G.G. Towell, J.W. Shavlik, Extracting refined rules from knowledge-based neural networks, Machine Learning. (1993). https://doi.org/10.1007/bf00993103.
[78]	S. Baik, M. Choi, J. Choi, H. Kim, K.M. Lee, Meta-learning with adaptive hyperparameters, in: Adv. Neural Inf. Process. Syst., 2020.
[79]	G. Ramos, C. Meek, P. Simard, J. Suh, S. Ghorashi, Interactive machine teaching: a human-centered approach to building machine-learned models, Human-Computer Interaction. (2020). https://doi.org/10.1080/07370024.2020.1734931.
[80]	X. Wang, Y. Chen, W. Zhu, A Survey on Curriculum Learning, IEEE Transactions on Pattern Analysis and Machine Intelligence. (2022). https://doi.org/10.1109/TPAMI.2021.3069908.
[81]	D. Bogdanova, M. Snoeck, Learning from errors: Error-based exercises in domain modelling pedagogy, in: Lect. Notes Bus. Inf. Process., 2018. https://doi.org/10.1007/978-3-030-02302-7_20.
[82]	A. Graves, M.G. Bellemare, J. Menick, R. Munos, K. Kavukcuoglu, Automated curriculum learning for neural networks, in: 34th Int. Conf. Mach. Learn. ICML 2017, 2017.
[83]	J.A.R. Rojas, M. Beth Kery, S. Rosenthal, A. Dey, Sampling techniques to improve big data exploration, in: 2017 IEEE 7th Symp. Large Data Anal. Vis. LDAV 2017, 2017. https://doi.org/10.1109/LDAV.2017.8231848.
[84]	T. Emmanuel, T. Maupong, D. Mpoeleng, T. Semong, B. Mphago, O. Tabona, A survey on missing data in machine learning, Journal of Big Data. (2021). https://doi.org/10.1186/s40537-021-00516-9.
[85]	F. Alharbi, K. El Hindi, S. Al-Ahmadi, Error-Based Noise Filtering during Neural Network Training, IEEE Access. (2020). https://doi.org/10.1109/ACCESS.2020.3019465.
[86]	Z. Yang, T. Huang, M. Ding, Y. Dong, R. Ying, Y. Cen, Y. Geng, J. Tang, BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs, in: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., 2023. https://doi.org/10.1145/3580305.3599263.
[87]	P. Dogga, K. Narasimhan, A. Sivaraman, S.K. Saini, G. Varghese, R. Netravali, Revelio: ML-Generated Debugging Queries for Finding Root Causes in Distributed Systems, Proceedings of Machine Learning and Systems. 4 (2022) pp. 601–622. https://github.com/ (accessed February 2, 2025).
[88]	A. Ramírez, Exploring Gender Bias in Misclassification with Clustering and Local Explanations, Communications in Computer and Information Science. 2135 CCIS (2025) pp. 136–151. https://doi.org/10.1007/978-3-031-74633-8_9.
[89]	M. Rana, M. Bhushan, Machine learning and deep learning approach for medical image analysis: diagnosis to detection, Multimedia Tools and Applications. (2023). https://doi.org/10.1007/s11042-022-14305-w.
[90]	R. Haffar, J. Domingo-Ferrer, D. Sánchez, Explaining misclassification and attacks in deep learning via random forests, in: Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), 2020. https://doi.org/10.1007/978-3-030-57524-3_23.
[91]	M.X. Cohen, C. Ranganath, Reinforcement learning signals predict future decisions, Journal of Neuroscience. (2007). https://doi.org/10.1523/JNEUROSCI.4421-06.2007.
[92]	D. Carneiro, M. Guimarães, M. Carvalho, P. Novais, Using meta-learning to predict performance metrics in machine learning problems, Expert Systems. (2023). https://doi.org/10.1111/exsy.12900.
[93]	J.P. Monteiro, D. Ramos, D. Carneiro, F. Duarte, J.M. Fernandes, P. Novais, Meta-learning and the new challenges of machine learning, International Journal of Intelligent Systems. (2021). https://doi.org/10.1002/int.22549.
[94]	M.C. Kennedy, A. O’Hagan, Bayesian calibration of computer models, Journal of the Royal Statistical Society. Series B: Statistical Methodology. (2001). https://doi.org/10.1111/1467-9868.00294.
[95]	T. Bansal, S. Alzubi, T. Wang, J.Y. Lee, A. McCallum, Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning, in: Proc. Mach. Learn. Res., 2022.
[96]	M.L. Littman, Reinforcement learning improves behaviour from evaluative feedback, Nature. (2015). https://doi.org/10.1038/nature14540.
[97]	J. Willard, X. Jia, S. Xu, M. Steinbach, V. Kumar, Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems, ACM Computing Surveys. (2022). https://doi.org/10.1145/3514228.
[98]	P. Karnakov, S. Litvinov, P. Koumoutsakos, Solving inverse problems in physics by optimizing a discrete loss: Fast and accurate learning without neural networks, PNAS Nexus. (2024). https://doi.org/10.1093/pnasnexus/pgae005.
[99]	A.M. Roy, S. Guha, A data-driven physics-constrained deep learning computational framework for solving von Mises plasticity, Engineering Applications of Artificial Intelligence. (2023). https://doi.org/10.1016/j.engappai.2023.106049.
[100]	M.J. Zideh, P. Chatterjee, A.K. Srivastava, Physics-Informed Machine Learning for Data Anomaly Detection, Classification, Localization, and Mitigation: A Review, Challenges, and Path Forward, IEEE Access. (2024). https://doi.org/10.1109/ACCESS.2023.3347989.
[101]	T. Dash, S. Chitlangia, A. Ahuja, A. Srinivasan, A review of some techniques for inclusion of domain-knowledge into deep neural networks, Scientific Reports. (2022). https://doi.org/10.1038/s41598-021-04590-0.
[102]	M.R. Pulicharla, Data Versioning and Its Impact on Machine Learning Models, Journal of Science & Technology. 5 (2024) pp. 22–37. https://doi.org/10.55662/JST.2024.5101.
[103]	R. Kurczab, S. Smusz, A.J. Bojarski, The influence of negative training set size on machine learning-based virtual screening, Journal of Cheminformatics. (2014). https://doi.org/10.1186/1758-2946-6-32.
[104]	G.C. Publio, A. Ławrynowicz, L. Soldatova, P. Panov, D. Esteves, J. Vanschoren, T. Soru, ML-Schema: An interchangeable format for description of machine learning experiments, Semantic Web Journal. (2019).
[105]	C. Aliferis, G. Simon, Overfitting, Underfitting and General Model Overconfidence and Under-Performance Pitfalls and Best Practices in Machine Learning and AI, in: 2024. https://doi.org/10.1007/978-3-031-39355-6_10.
[106]	C. Garbin, O. Marques, Assessing Methods and Tools to Improve Reporting, Increase Transparency, and Reduce Failures in Machine Learning Applications in Health Care, Radiology: Artificial Intelligence. (2022). https://doi.org/10.1148/ryai.210127.
[107]	I.Y. Chen, F.D. Johansson, D. Sontag, Why is my classifier discriminatory?, in: Adv. Neural Inf. Process. Syst., 2018.
[108]	A. Beutel, J. Chen, T. Doshi, H. Qian, A. Woodruff, C. Luu, P. Kreitmann, J. Bischof, E.H. Chi, Putting fairness principles into practice: Challenges, metrics, and improvements, in: AIES 2019 - Proc. 2019 AAAI/ACM Conf. AI, Ethics, Soc., 2019. https://doi.org/10.1145/3306618.3314234.
[109]	J.P. Lalor, A. Abbasi, K. Oketch, Y. Yang, N. Forsgren, Should Fairness be a Metric or a Model? A Model-based Framework for Assessing Bias in Machine Learning Pipelines, ACM Transactions on Information Systems. 42 (2024) pp. 41. https://doi.org/10.1145/3641276/ASSET/931CE00F-6A98-45B5-AC2D-EF0C1756F756/ASSETS/GRAPHIC/TOIS-2023-0096-F13.JPG.
[110]	S. Dey, S.W. Lee, Multilayered review of safety approaches for machine learning-based systems in the days of AI, Journal of Systems and Software. (2021). https://doi.org/10.1016/j.jss.2021.110941.
[111]	R. Elshawi, S. Sakr, D. Talia, P. Trunfio, Big Data Systems Meet Machine Learning Challenges: Towards Big Data Science as a Service, Big Data Research. (2018). https://doi.org/10.1016/j.bdr.2018.04.004.
[112]	F. Semeraro, A. Griffiths, A. Cangelosi, Human–robot collaboration and machine learning: A systematic review of recent research, Robotics and Computer-Integrated Manufacturing. (2022). https://doi.org/10.1016/j.rcim.2022.102432.
[113]	D. Donoho, 50 Years of Data Science, Journal of Computational and Graphical Statistics. (2017). https://doi.org/10.1080/10618600.2017.1384734.
[114]	C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, Nature Machine Intelligence. (2019). https://doi.org/10.1038/s42256-019-0048-x.
[115]	O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg, L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision. (2015). https://doi.org/10.1007/s11263-015-0816-y.
[116]	Z. Shang, E. Zgraggen, B. Buratti, F. Kossmann, P. Eichmann, Y. Chung, C. Binnig, E. Upfal, T. Kraska, Democratizing data science through interactive curation of ML pipelines, Proceedings of the ACM SIGMOD International Conference on Management of Data. (2019) pp. 1171–1188. https://doi.org/10.1145/3299869.3319863.
[117]	A. Mumuni, F. Mumuni, Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods, Knowledge and Information Systems 2025. (2024) pp. 1–51. https://doi.org/10.1007/S10115-025-02349-X/TABLES/4.
[118]	V. Cerqueira, H.M. Gomes, A. Bifet, L. Torgo, STUDD: a student–teacher method for unsupervised concept drift detection, Machine Learning. (2023). https://doi.org/10.1007/s10994-022-06188-7.
[119]	H. Jeong, S. Barbara, M.D. Wu, N. Dasgupta, U. Amherst, M.M. Mit, F. Calmon, Who Gets the Benefit of the Doubt? Racial Bias in Machine Learning Algorithms Applied to Secondary School Math Education, (n.d.).
[120]	Q. Zhao, T. Hastie, Causal Interpretations of Black-Box Models, Journal of Business and Economic Statistics. (2021). https://doi.org/10.1080/07350015.2019.1624293.
[121]	B. Schölkopf, FastForwardLabs, B. Schölkopf, Causality for Machine Learning, Probabilistic and Causal Inference. (2022). https://doi.org/10.1145/3501714.3501755.
[122]	A.T. Nguyen, P.H.S. Torr, S.N. Lim, FedSR: A Simple and Effective Domain Generalization Method for Federated Learning, in: Adv. Neural Inf. Process. Syst., 2022.
[123]	S. Qiu, M. Wang, Y. Yang, G. Yu, J. Wang, Z. Yan, C. Domeniconi, M. Guo, Meta Multi-Instance Multi-Label learning by heterogeneous network fusion, Information Fusion. (2023). https://doi.org/10.1016/j.inffus.2023.02.010.
[124]	L. Snidaro, J. García, J. Llinas, Context-based Information Fusion: A survey and discussion, Information Fusion. (2015). https://doi.org/10.1016/j.inffus.2015.01.002.
[125]	D. Dubois, W. Liu, J. Ma, H. Prade, The basic principles of uncertain information fusion. An organised review of merging rules in different representation frameworks, Information Fusion. (2016). https://doi.org/10.1016/j.inffus.2016.02.006.
